#!/usr/bin/env python3
"""
SAIT_01 Production Model Deployment Script
Deploys the validated Battlefield Model as the primary production model
"""

import os
import sys
import numpy as np
import tensorflow as tf
from tensorflow import keras
import json
from pathlib import Path
import time

sys.path.append('.')
from sait01_model_architecture import SaitAudioPreprocessor

class ProductionModelDeployment:
    """Production deployment for SAIT_01 Battlefield Model"""
    
    def __init__(self):
        self.model_path = "sait01_production_model.h5"
        self.preprocessor = SaitAudioPreprocessor()
        self.class_names = ['background', 'vehicle', 'aircraft']
        
        # Production thresholds based on validation
        self.confidence_thresholds = {
            'high': 0.95,      # Use prediction directly
            'medium': 0.75,    # Acceptable with monitoring  
            'low': 0.75        # Flag for review
        }
        
        # Performance specifications
        self.specs = {
            'validated_accuracy': 0.8683,
            'parameters': 167939,
            'inference_time_ms': 0.56,
            'model_size_mb': 2.1,
            'input_shape': (64, 63, 1),
            'classes': 3
        }
    
    def load_production_model(self):
        """Load and verify production model"""
        print("üöÄ LOADING SAIT_01 PRODUCTION MODEL")
        print("=" * 50)
        
        if not os.path.exists(self.model_path):
            print(f"‚ùå Production model not found: {self.model_path}")
            return None
        
        try:
            # Load model
            model = keras.models.load_model(self.model_path, compile=False)
            
            # Compile for production
            model.compile(
                optimizer=keras.optimizers.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            print(f"‚úÖ Production model loaded successfully")
            print(f"   File: {self.model_path}")
            print(f"   Parameters: {model.count_params():,}")
            print(f"   Input shape: {model.input_shape}")
            print(f"   Validated accuracy: {self.specs['validated_accuracy']*100:.2f}%")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Failed to load production model: {e}")
            return None
    
    def verify_model_performance(self, model, test_samples=100):
        """Quick performance verification"""
        print(f"\nüîç VERIFYING MODEL PERFORMANCE")
        print("-" * 40)
        
        # Generate test samples
        print(f"Generating {test_samples} test samples...")
        test_data = np.random.randn(test_samples, 64, 63, 1)
        
        # Test inference speed
        start_time = time.time()
        predictions = model.predict(test_data, verbose=0)
        inference_time = time.time() - start_time
        
        avg_inference_ms = (inference_time / test_samples) * 1000
        
        print(f"‚úÖ Performance verification complete:")
        print(f"   Inference time: {avg_inference_ms:.2f}ms per sample")
        print(f"   Expected: {self.specs['inference_time_ms']:.2f}ms per sample")
        print(f"   Batch processing: {inference_time:.3f}s for {test_samples} samples")
        
        # Verify output format
        pred_classes = np.argmax(predictions, axis=1)
        confidences = np.max(predictions, axis=1)
        
        print(f"   Output format: ‚úÖ Valid")
        print(f"   Prediction range: {pred_classes.min()}-{pred_classes.max()}")
        print(f"   Confidence range: {confidences.min():.3f}-{confidences.max():.3f}")
        
        return True
    
    def create_production_inference_function(self, model):
        """Create optimized inference function for production"""
        
        def production_inference(audio_file_path):
            """
            Production inference function for SAIT_01
            
            Args:
                audio_file_path: Path to audio file
                
            Returns:
                dict: Prediction results with confidence and metadata
            """
            try:
                # Load and preprocess audio
                audio = self.preprocessor.load_and_resample(audio_file_path)
                mel_spec = self.preprocessor.extract_mel_spectrogram(audio)
                
                # Ensure correct input shape
                if len(mel_spec.shape) == 2:
                    mel_spec = np.expand_dims(mel_spec, axis=-1)
                
                # Add batch dimension
                input_data = np.expand_dims(mel_spec, axis=0)
                
                # Run inference
                start_time = time.time()
                prediction = model.predict(input_data, verbose=0)[0]
                inference_time_ms = (time.time() - start_time) * 1000
                
                # Get results
                predicted_class_idx = np.argmax(prediction)
                confidence = float(prediction[predicted_class_idx])
                predicted_class = self.class_names[predicted_class_idx]
                
                # Determine confidence level
                if confidence >= self.confidence_thresholds['high']:
                    confidence_level = 'high'
                elif confidence >= self.confidence_thresholds['medium']:
                    confidence_level = 'medium'
                else:
                    confidence_level = 'low'
                
                return {
                    'prediction': predicted_class,
                    'confidence': confidence,
                    'confidence_level': confidence_level,
                    'class_probabilities': {
                        self.class_names[i]: float(prediction[i]) 
                        for i in range(len(self.class_names))
                    },
                    'inference_time_ms': inference_time_ms,
                    'model_version': 'battlefield_v1.0'
                }
                
            except Exception as e:
                return {
                    'error': str(e),
                    'prediction': None,
                    'confidence': 0.0,
                    'confidence_level': 'error'
                }
        
        return production_inference
    
    def test_production_inference(self, inference_function):
        """Test production inference with sample data"""
        print(f"\nüß™ TESTING PRODUCTION INFERENCE")
        print("-" * 40)
        
        # Check for sample audio files
        dataset_dir = Path("massive_enhanced_dataset")
        test_files = []
        
        for class_name in self.class_names:
            class_dir = dataset_dir / class_name
            if class_dir.exists():
                files = list(class_dir.glob("*.wav"))[:2]  # 2 samples per class
                test_files.extend(files)
        
        if len(test_files) == 0:
            print("‚ö†Ô∏è  No test audio files found, skipping inference test")
            return
        
        print(f"Testing inference on {len(test_files)} sample files...")
        
        for i, test_file in enumerate(test_files[:6]):  # Test max 6 files
            print(f"\nüìù Test {i+1}: {test_file.name}")
            
            result = inference_function(str(test_file))
            
            if 'error' in result:
                print(f"   ‚ùå Error: {result['error']}")
            else:
                print(f"   Prediction: {result['prediction']}")
                print(f"   Confidence: {result['confidence']:.3f} ({result['confidence_level']})")
                print(f"   Inference time: {result['inference_time_ms']:.2f}ms")
        
        print(f"\n‚úÖ Production inference testing complete")
    
    def generate_deployment_package(self, model, inference_function):
        """Generate deployment package with model and utilities"""
        print(f"\nüì¶ GENERATING DEPLOYMENT PACKAGE")
        print("-" * 40)
        
        # Create deployment directory
        deploy_dir = Path("sait01_production_deployment")
        deploy_dir.mkdir(exist_ok=True)
        
        # Copy production model
        model.save(deploy_dir / "sait01_production_model.h5")
        print(f"‚úÖ Model saved: {deploy_dir}/sait01_production_model.h5")
        
        # Create deployment configuration
        config = {
            'model_info': {
                'name': 'SAIT_01_Battlefield_Model',
                'version': '1.0',
                'validated_accuracy': self.specs['validated_accuracy'],
                'parameters': self.specs['parameters'],
                'inference_time_ms': self.specs['inference_time_ms'],
                'deployment_date': str(np.datetime64('today'))
            },
            'audio_config': {
                'sample_rate': 16000,
                'window_ms': 1000,
                'n_mels': 64,
                'n_frames': 63,
                'input_shape': list(self.specs['input_shape'])
            },
            'classes': {
                'names': self.class_names,
                'count': len(self.class_names)
            },
            'confidence_thresholds': self.confidence_thresholds,
            'performance_targets': {
                'inference_time_max_ms': 10.0,
                'accuracy_minimum': 0.85,
                'memory_usage_max_mb': 5.0
            }
        }
        
        with open(deploy_dir / "deployment_config.json", 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"‚úÖ Config saved: {deploy_dir}/deployment_config.json")
        
        # Create Python inference module
        inference_code = f'''#!/usr/bin/env python3
"""
SAIT_01 Production Inference Module
Battlefield Model v1.0 - Validated 86.83% Accuracy
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import json
import time
import sys
from pathlib import Path

# Add path for preprocessor
sys.path.append('.')
from sait01_model_architecture import SaitAudioPreprocessor

class SAIT01ProductionInference:
    def __init__(self, model_path="sait01_production_model.h5", config_path="deployment_config.json"):
        # Load configuration
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        # Load model
        self.model = keras.models.load_model(model_path, compile=False)
        self.preprocessor = SaitAudioPreprocessor()
        self.class_names = self.config['classes']['names']
        
        print(f"SAIT_01 Production Model Loaded")
        print(f"Validated Accuracy: {{self.config['model_info']['validated_accuracy']*100:.2f}}%")
    
    def predict(self, audio_file_path):
        """Run production inference on audio file"""
        try:
            # Load and preprocess
            audio = self.preprocessor.load_and_resample(audio_file_path)
            mel_spec = self.preprocessor.extract_mel_spectrogram(audio)
            
            if len(mel_spec.shape) == 2:
                mel_spec = np.expand_dims(mel_spec, axis=-1)
            
            input_data = np.expand_dims(mel_spec, axis=0)
            
            # Inference
            start_time = time.time()
            prediction = self.model.predict(input_data, verbose=0)[0]
            inference_time_ms = (time.time() - start_time) * 1000
            
            # Results
            predicted_class_idx = np.argmax(prediction)
            confidence = float(prediction[predicted_class_idx])
            predicted_class = self.class_names[predicted_class_idx]
            
            return {{
                'prediction': predicted_class,
                'confidence': confidence,
                'inference_time_ms': inference_time_ms,
                'probabilities': {{self.class_names[i]: float(prediction[i]) for i in range(len(self.class_names))}}
            }}
            
        except Exception as e:
            return {{'error': str(e)}}

# Example usage
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python sait01_inference.py <audio_file_path>")
        sys.exit(1)
    
    model = SAIT01ProductionInference()
    result = model.predict(sys.argv[1])
    
    if 'error' in result:
        print(f"Error: {{result['error']}}")
    else:
        print(f"Prediction: {{result['prediction']}}")
        print(f"Confidence: {{result['confidence']:.3f}}")
        print(f"Inference time: {{result['inference_time_ms']:.2f}}ms")
'''
        
        with open(deploy_dir / "sait01_inference.py", 'w') as f:
            f.write(inference_code)
        
        print(f"‚úÖ Inference module: {deploy_dir}/sait01_inference.py")
        
        # Create README
        readme = f'''# SAIT_01 Production Deployment Package

## Model Specifications
- **Model**: Battlefield Model v1.0
- **Validated Accuracy**: {self.specs['validated_accuracy']*100:.2f}%
- **Parameters**: {self.specs['parameters']:,}
- **Inference Time**: {self.specs['inference_time_ms']:.2f}ms per sample
- **Model Size**: {self.specs['model_size_mb']:.1f}MB

## Files
- `sait01_production_model.h5` - Production TensorFlow model
- `deployment_config.json` - Configuration and specifications
- `sait01_inference.py` - Python inference module
- `README.md` - This file

## Usage
```python
from sait01_inference import SAIT01ProductionInference

model = SAIT01ProductionInference()
result = model.predict("audio_file.wav")
print(f"Prediction: {{result['prediction']}}")
```

## Command Line
```bash
python sait01_inference.py audio_file.wav
```

## Performance Targets
- Inference time: <10ms per sample
- Memory usage: <5MB
- Minimum accuracy: 85% (achieved: 86.83%)

## Deployment Date
{np.datetime64('today')}
'''
        
        with open(deploy_dir / "README.md", 'w') as f:
            f.write(readme)
        
        print(f"‚úÖ Documentation: {deploy_dir}/README.md")
        print(f"\nüéØ Deployment package complete: {deploy_dir}/")
        
        return deploy_dir
    
    def deploy(self):
        """Complete production deployment process"""
        print("üöÄ SAIT_01 PRODUCTION MODEL DEPLOYMENT")
        print("=" * 60)
        
        # Load production model
        model = self.load_production_model()
        if not model:
            return False
        
        # Verify performance
        if not self.verify_model_performance(model):
            return False
        
        # Create inference function
        inference_function = self.create_production_inference_function(model)
        
        # Test inference
        self.test_production_inference(inference_function)
        
        # Generate deployment package
        deploy_dir = self.generate_deployment_package(model, inference_function)
        
        print(f"\n‚úÖ PRODUCTION DEPLOYMENT SUCCESSFUL")
        print(f"   Primary model: sait01_production_model.h5")
        print(f"   Validated accuracy: {self.specs['validated_accuracy']*100:.2f}%")
        print(f"   Deployment package: {deploy_dir}/")
        print(f"   Status: READY FOR SAIT_01 INTEGRATION")
        
        return True

def main():
    deployer = ProductionModelDeployment()
    success = deployer.deploy()
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())